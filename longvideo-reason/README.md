# LongVideo-Reason

## Data Generation Process
We provide scripts for generating the LongVideo-Reason dataset. If you’d like to generate reasoning data on your own videos, follow the instructions below. Otherwise, feel free to skip this section.
- **Step 1: Split long videos into short clips**
  - `pip install moviepy`
  - `conda install ffmpeg`
  - Use the following script to generate short clips. `--input_dir` is the folder contains the original videos, `--output_dir` is the folder where the short clips will be generated. `clip_duration` is the second that each short clip will have, we usually set it as `10`.
  - `python step1_split_videos.py --input_dir $INPUT_DIR --output_dir $OUTPUT_DIR --clip_duration 10`
- **Step 2: Generate video captions for each short clips**
  - We use `Efficient-Large-Model/NVILA-8B-Video` for caption generation. It requires to install the VILA repo environment, install instruction at [here](https://github.com/NVlabs/VILA/blob/main/environment_setup.sh). You can change it to others and modify the `step2_gen_video_captions.py` accordingly, if you want to use other VLMs for captioning.
  - Please use the following script for generation. `--model_path` is the model path. `--video_dir` is the folder contains the short clips. `--output_dir` is the output folder where the captions will be generated. Note that this step will cost large amount of GPU-hours if your video amount is large.
  - `python step2_gen_video_captions.py --model_path $MODEL_PATH --video_dir $VIDEO_DIR --output_dir $OUTPUT_DIR`
- **Step 3: Mege these captions**
  - Please use the following script to merge the generated captions into a json file. `--input_dir` is the output folder that contains the generated captions. `--output_file` should the merged json file, like `merged.json`.
  - `python step3_merge_captions.py --input_dir $INPUT_DIR --output_file $OUTPUT_FILE`
- **Step 4: Generate reasoning data with a reasoning LLM**
  - We provide a slurm script `slurm_gen_reasoning_data.sh` to use the `deepseek-ai/DeepSeek-R1` to model to generate the reasoning data. Please modify the `model_path`, `captions`, and `output_folder` accordingly in the script. Note that this step will also cost large amount of GPU-hours. The minimum GPU requirements are 2 H100 nodes or 4 A100 nodes, 8 GPU for each node.
- **Step 5: Parse the generated reasoning data**
  - We need to parse the generated reasoning data into a trainable format. Please use the following script. `--input_dir` is the folder that contains the generated reasoning data in Step 4. `--output_file` is the output json file, like `parsed_reasoning_data.json`.
  - `python step5_parse_reasoning_data.py --input_dir $INPUT_DIR --output_file $PARSED_DATA_JSON`
- **Step 6 (Optional): Reformat the reasoning steps**
  - Since the reasoning generated by LLMs can sometimes be unnatural, making it less suitable for direct training, we can reformat it to improve the fluency of the language and the clarity of the logic. Please follow the following script. `--input_json` is the generated json file in Step 5. `--output_dir` is the output folder.
  - `python step6_reformat_reasoning_data.py --input_json $INPUT_JSON --output_dir $OUTPUT_DIR`
  - Note that this step is optional, you can also directly concatenate the output data in Step 5 for training directly. You can then merge the output json files in the `OUTPUT_DIR` into a jsonl file for training then.

## Testing on LongVideo-Reason-eval
In this section, we release the scripts for testing on our LongVideo-Reason-eval set. More details about the training set can be found [here](https://github.com/NVlabs/Long-RL/issues/1).

You can find the videos for testing [here](https://huggingface.co/datasets/LongVideo-Reason/longvideo_eval_videos/tree/main). Please download them, and `tar -zxvf` them into a directory named `longvila_videos`.
```
├── $VIDEO_DIR
│   ├── longvila_videos
│   │   │── mp4/webm/mkv videos
```


`$VIDEO_DIR` is the parent directory of `longvila_videos`. For different models, you need to customize the `model_generate` function accordingly. The model generations and output metrics will be saved in `runs_${$MODEL_PATH}`.
```bash
python eval.py \
        --model-path $MODEL_PATH \
        --data-path LongVideo-Reason/longvideo-reason@test \
        --video-dir $VIDEO_DIR \
        --output-dir runs_${$MODEL_PATH}
```

## Citation
Please consider to cite our paper if this benchmark are helpful in your research.

```bibtex
@article{chen2025longvila-r1,
      title={Scaling RL to Long Videos},
      author={Yukang Chen and Wei Huang and Baifeng Shi and Qinghao Hu and Hanrong Ye and Ligeng Zhu and Zhijian Liu and Pavlo Molchanov and Jan Kautz and Xiaojuan Qi and Sifei Liu and Hongxu Yin and Yao Lu and Song Han},
      year={2025},
      eprint={2507.07966},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```
